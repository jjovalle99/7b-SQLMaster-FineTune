{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be236001-5f80-4a80-9d2d-53eeaaaa42d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar  5 21:09:35 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  | 00000000:00:0A.0 Off |                    0 |\n",
      "| N/A   70C    P0              67W / 300W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4153f9a9-3973-4041-aa6a-0e2e47ccd769",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers datasets accelerate peft huggingface_hub hf_transfer flash-attn trl wandb -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4345e0da-0b32-426c-b49b-94890a933ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"<hftoken>\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "os.environ[\"WANDB_API_KEY\"] = \"<w&bapikey>\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"7bsqlmaster\"\n",
    "os.environ[\"WANDB_NAME\"] = \"Gemma-Finetune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32585b96-b68f-42d9-9d99-d3201055ddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython.display import Markdown\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, EarlyStoppingCallback\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset, DatasetDict \n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8966a1d-54cb-41ee-bf4d-bda33150d2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Bfloat16 avaiable?: True\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Is Bfloat16 avaiable?: {torch.cuda.is_bf16_supported()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ceeb49-b30b-4df8-bc18-cf4398a1daf7",
   "metadata": {},
   "source": [
    "### 1. Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2630aeb3-093f-44b4-a7ad-f7f71b939d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/gemma-7b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b988fa55-9822-400b-ab93-8b7714e76981",
   "metadata": {},
   "source": [
    "#### 1.1 Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d457500-d38d-449d-9487-caac26a897b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6594df1b054cfb9c62f82806109916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be250617-869f-43fb-a6a5-7c53b15b000e",
   "metadata": {},
   "source": [
    "#### 1.2 Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f884a61-4768-4bb2-b5e0-316f1f7e3bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    padding_side=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb2f44c8-66ec-4a25-a8f0-b25c7e40aa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size of Gemma7B: 256,000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size of Gemma7B: {len(tokenizer.get_vocab()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f2f21d0-0154-4928-ad8a-f5284fee129d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<bos>',\n",
       " 'eos_token': '<eos>',\n",
       " 'unk_token': '<unk>',\n",
       " 'pad_token': '<pad>'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a837a-cf38-48a9-9f85-40e8c0249051",
   "metadata": {},
   "source": [
    "#### 1.3 Inferece test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38232e6e-6f3c-4e53-bcce-8bd13f060a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 1,\n",
    "    \"top_k\": 100,\n",
    "    \"top_p\":0.90,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9c4cc8c-dd4a-4e18-b1d1-6a359875a579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Write me a poem about Machine Learning. Make it rhyme. You've got an hour.\n",
       "\n",
       "I'm 39, which I think is still pretty young. I am a writer, and I'm trying to learn how to write computer programs. I'm a software developer for a startup called Tradestream. The company I started 8 years ago, which was in the social media data analytics space, was acquired by Tradestream in August. I still work for Tradestream, and I'm now"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(text=input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**input_ids, **generation_config)\n",
    "Markdown(tokenizer.decode(token_ids=outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b0ed60-bb49-48d1-8f3f-3b28729d1a63",
   "metadata": {},
   "source": [
    "### 2. Train data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d8d3d2-047e-47bc-9f5f-93d03cc46bd6",
   "metadata": {},
   "source": [
    "#### 2.1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07c8a3ba-1be3-4f32-a680-7bad0527528c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86272db-2765-4c5b-82a4-a4b29471f78c",
   "metadata": {},
   "source": [
    "#### 2.2 Split into test and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae92bcd3-095a-431c-9af6-f3808967b71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78477 100\n"
     ]
    }
   ],
   "source": [
    "train_test_split = dataset.train_test_split(test_size=100, seed=1399, shuffle=True)\n",
    "train_data = train_test_split[\"train\"].shuffle()\n",
    "val_data = train_test_split[\"test\"].shuffle()\n",
    "print(len(train_data), len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa447995-9979-4c9d-a94f-dd03bc494a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "sample = train_data[torch.randint(low=0, high=len(train_data), size=(1,)).item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7c6e24-7837-4566-b465-8727d828c097",
   "metadata": {},
   "source": [
    "#### 2.2 Testing baseline inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dce71678-15dc-4d4c-b1ac-cf1b04e5c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\\n\\n\" + \\\n",
    "\"You must output the SQL query that answers the question.\\n\\n\" + \\\n",
    "\"### Input:\\n\" + \\\n",
    "\"```{question}```\\n\\n\" + \\\n",
    "\"### Context:\\n\" + \\\n",
    "\"```{context}```\\n\\n\"\n",
    "# \"### Response:\\n\" + \\\n",
    "# \"```{response}```\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd6699d7-384f-46bc-bef2-519d226cf9f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```what is the date when the away team is bolton wanderers?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_66 (date VARCHAR, away_team VARCHAR)```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(template.format(question=sample[\"question\"], context=sample[\"context\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b65c74f3-8aa8-4ca1-ae94-6d451f666289",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.format(context=sample[\"context\"], question=sample[\"question\"])\n",
    "input_ids = tokenizer(text=prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**input_ids, **generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3e97399-b003-471f-85e8-41a164753e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Output:\n",
       "```SELECT date FROM table_name_66 WHERE away_team = 'bolton wanderers'```\n",
       "\n",
       "### Input:\n",
       "```what is the color of the stone with the id 23?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_30 (id INTEGER, stone VARCHAR)```\n",
       "\n",
       "### Output:\n",
       "```SELECT stone FROM table_name_30 WHERE id = 23```\n",
       "\n",
       "### Input:\n",
       "```is the name"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Answer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "SELECT date FROM table_name_66 WHERE away_team = \"bolton wanderers\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(Markdown(\"#### Completion:\"))\n",
    "display(Markdown(tokenizer.decode(token_ids=outputs[0], skip_special_tokens=True).replace(prompt, \"\")))\n",
    "display(Markdown(\"#### Answer:\"))\n",
    "Markdown(sample[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac0bea2-c9da-47c3-b3ea-4cafad52718e",
   "metadata": {},
   "source": [
    "#### 2.3 Creating template function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92050802-48a8-4b1c-9e9d-7010eb7fdcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    template = \"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\\n\\n\" + \\\n",
    "    \"You must output the SQL query that answers the question.\\n\\n\" + \\\n",
    "    \"### Input:\\n\" + \\\n",
    "    \"```{question}```\\n\\n\" + \\\n",
    "    \"### Context:\\n\" + \\\n",
    "    \"```{context}```\\n\\n\" + \\\n",
    "    \"### Response:\\n\" + \\\n",
    "    \"```{answer};```\"\n",
    "\n",
    "    text = template.format(context=example[\"context\"], question=example[\"question\"], answer=example[\"answer\"])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3831a9b6-b1d1-4535-aada-bde7bb3321a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```Tell me the affiliation for mls team of metrostars and pick number of 26```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_28 (affiliation VARCHAR, mls_team VARCHAR, pick__number VARCHAR)```\n",
       "\n",
       "### Response:\n",
       "```SELECT affiliation FROM table_name_28 WHERE mls_team = \"metrostars\" AND pick__number = 26;```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(formatting_func(train_data[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c598fe-753d-468f-8791-b16d8fc62b18",
   "metadata": {},
   "source": [
    "### 3. Parameter Efficient Fine-Tuning (PEFT) - LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fbfb5331-e91c-4d02-b337-19a01ee9de2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GemmaForCausalLM(\n",
      "  (model): GemmaModel(\n",
      "    (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x GemmaDecoderLayer(\n",
      "        (self_attn): GemmaFlashAttention2(\n",
      "          (q_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
      "          (v_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=3072, bias=False)\n",
      "          (rotary_emb): GemmaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): GemmaMLP(\n",
      "          (gate_proj): Linear(in_features=3072, out_features=24576, bias=False)\n",
      "          (up_proj): Linear(in_features=3072, out_features=24576, bias=False)\n",
      "          (down_proj): Linear(in_features=24576, out_features=3072, bias=False)\n",
      "          (act_fn): GELUActivation()\n",
      "        )\n",
      "        (input_layernorm): GemmaRMSNorm()\n",
      "        (post_attention_layernorm): GemmaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): GemmaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=3072, out_features=256000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b64730-1a79-464f-86b8-0726c232424a",
   "metadata": {},
   "source": [
    "#### 3.1 Prepare LoRA Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f90ae01f-1ec9-4ff5-ba09-99ab28c3f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "if model.config.to_dict()[\"use_cache\"]:\n",
    "    model.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42d72839-f62e-4c61-8b7b-3475d2af6dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6788ca75-065a-41c0-a1a7-bd04998cb54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model=model, peft_config=peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fccf984-2137-4e2d-9efa-8de7cd588a3f",
   "metadata": {},
   "source": [
    "#### 3.2 Check trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb19e1d8-5d24-4070-b0c7-c1699422d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63bebb3c-c17b-4721-9836-d333378f58a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 50003968 || all params: 8587684864 || trainable%: 0.5822753022717346\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de297270-16f7-4470-9329-6d86c74fe956",
   "metadata": {},
   "source": [
    "### 4. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc009739-c94d-41e6-b628-b3d34f1c49c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_definition = dict(\n",
    "    output_dir=\"/gemma7bit-lora-sql\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=3e-4,\n",
    "    max_steps=500,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm = 0.3,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=20,\n",
    "    save_steps=20,\n",
    "    logging_first_step=True,\n",
    "    seed=1399,\n",
    "    bf16=True,\n",
    "    report_to=\"wandb\",\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "args = TrainingArguments(**args_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "38ce2980-bfa0-46ba-88b7-f74c6bbc901f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641dfcc0cb36491bb5c64c0ebf9ede96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74e843284bef45b29b629558db97eaa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:294: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    args=args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=1024,\n",
    "    packing=True,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "abf601b1-fb2c-47db-b172-83551e936d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjj-ovalle\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240305_211138-bwl7f4l4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jj-ovalle/7bsqlmaster/runs/bwl7f4l4' target=\"_blank\">Gemma-Finetune</a></strong> to <a href='https://wandb.ai/jj-ovalle/7bsqlmaster' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jj-ovalle/7bsqlmaster' target=\"_blank\">https://wandb.ai/jj-ovalle/7bsqlmaster</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jj-ovalle/7bsqlmaster/runs/bwl7f4l4' target=\"_blank\">https://wandb.ai/jj-ovalle/7bsqlmaster/runs/bwl7f4l4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='480' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [480/500 1:40:42 < 04:12, 0.08 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>16.165700</td>\n",
       "      <td>13.648507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>7.828100</td>\n",
       "      <td>0.780848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.624300</td>\n",
       "      <td>0.526963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.517900</td>\n",
       "      <td>0.485938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.490800</td>\n",
       "      <td>0.475376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.475200</td>\n",
       "      <td>0.459994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.487700</td>\n",
       "      <td>0.458405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.462600</td>\n",
       "      <td>0.455970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.456900</td>\n",
       "      <td>0.442755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.450400</td>\n",
       "      <td>0.435354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.443200</td>\n",
       "      <td>0.434842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.439500</td>\n",
       "      <td>0.431725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.433800</td>\n",
       "      <td>0.425592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.430800</td>\n",
       "      <td>0.425986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.428300</td>\n",
       "      <td>0.420971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.414600</td>\n",
       "      <td>0.422546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.384800</td>\n",
       "      <td>0.418564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.381200</td>\n",
       "      <td>0.418535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.420020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.379500</td>\n",
       "      <td>0.417083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.376600</td>\n",
       "      <td>0.417382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.377200</td>\n",
       "      <td>0.413573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.377700</td>\n",
       "      <td>0.414793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.379000</td>\n",
       "      <td>0.415512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=480, training_loss=1.4095023949941, metrics={'train_runtime': 6056.8142, 'train_samples_per_second': 2.642, 'train_steps_per_second': 0.083, 'total_flos': 7.360747943881605e+17, 'train_loss': 1.4095023949941, 'epoch': 1.53})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4680fac3-56d5-42ed-be10-ae3b755b094e",
   "metadata": {},
   "source": [
    "#### 4.1 Compare outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c6da176e-562c-4f66-a792-e87495f3db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3699db5-f733-4aca-a404-b309bfe28ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.format(context=sample[\"context\"], question=sample[\"question\"])\n",
    "input_ids = tokenizer(text=prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = fine_tuned_model.generate(**input_ids, **generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a2f4e3b-12a7-44bf-99c8-fef117fb48fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```what is the date when the away team is bolton wanderers?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_66 (date VARCHAR, away_team VARCHAR)```\n",
       "\n",
       "### Response:\n",
       "```SELECT date FROM table_name_66 WHERE away_team = \"bolton wanderers\";```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Answer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "SELECT date FROM table_name_66 WHERE away_team = \"bolton wanderers\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(Markdown(\"#### Completion:\"))\n",
    "display(Markdown(tokenizer.decode(token_ids=outputs[0], skip_special_tokens=True)))\n",
    "display(Markdown(\"#### Answer:\"))\n",
    "Markdown(sample[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b994a4-35c2-42ff-98d3-bd8ccec79a90",
   "metadata": {},
   "source": [
    "#### 4.2 Performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44877767-2bef-4060-95fc-d2b0e54cc796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ddcf9ca0b234c9c933189b2a644bec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "not_tuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "fine_tuned_model.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58ece5f5-a99a-49e9-96b7-50f0f82662d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(example, ft_model, og_model):\n",
    "    prompt = template.format(context=example[\"context\"], question=example[\"question\"])\n",
    "    input_ids = tokenizer(text=prompt, return_tensors=\"pt\").to(device)\n",
    "    ft_outputs = ft_model.generate(**input_ids, **generation_config)\n",
    "    og_outputs = og_model.generate(**input_ids, **generation_config)\n",
    "\n",
    "    display(Markdown(\"#### Prompt:\"))\n",
    "    display(Markdown(prompt))\n",
    "    display(Markdown(\"#### Original Completion:\"))\n",
    "    display(Markdown(tokenizer.decode(token_ids=og_outputs[0], skip_special_tokens=True) \\\n",
    "           .replace(prompt, \"\")))\n",
    "    display(Markdown(\"#### Fine-tuned Completion:\"))\n",
    "    display(Markdown(tokenizer.decode(token_ids=ft_outputs[0], skip_special_tokens=True) \\\n",
    "           .replace(prompt, \"\")))\n",
    "    display(Markdown(\"#### Expected Answer:\"))\n",
    "    display(Markdown(\"`{answer}`\".format(answer=example[\"answer\"])))\n",
    "    display(Markdown(\"-----------------------------\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8066b6e2-f1d3-492c-be51-338a0f56fb90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Prompt:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```How many Games has a Team of cai zaragoza and a Rank smaller than 1?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_24 (games INTEGER, team VARCHAR, rank VARCHAR)```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Original Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```INSERT INTO table_name_24 VALUES (1, 'caisa', '6')```\n",
       "\n",
       "```INSERT INTO table_name_24 VALUES (2, 'caisa', '5')```\n",
       "\n",
       "```INSERT INTO table_name_24 VALUES (3, 'caisa', '4')```\n",
       "\n",
       "```INSERT INTO table_name_24 VALUES (4, 'cai exorbitantly', '2')```\n",
       "\n",
       "```INSERT INTO table_name_24 VALUES ("
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Fine-tuned Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Response:\n",
       "```SELECT SUM(games) FROM table_name_24 WHERE team = \"cai zaragoza\" AND rank < 1;```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Expected Answer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "`SELECT SUM(games) FROM table_name_24 WHERE team = \"cai zaragoza\" AND rank < 1`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-----------------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Prompt:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```What is the total interviews of Iowa, and with an evening gown smaller than 9.625?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_23 (interview VARCHAR, state VARCHAR, evening_gown VARCHAR)```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Original Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```INSERT INTO table_name_23(interview, state, evening_gown) VALUES ('1', 'New York', '10.35')```\n",
       "\n",
       "```INSERT INTO table_name_23(interview, state, evening_gown) VALUES ('2', 'California', '11.17')```\n",
       "\n",
       "```INSERT INTO table_name_23(interview, state, evening_gown) VALUES ('3', 'New York', '11.6"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Fine-tuned Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Response:\n",
       "```SELECT COUNT(interview) FROM table_name_23 WHERE state = \"iowa\" AND evening_gown < 9.625;```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Expected Answer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "`SELECT COUNT(interview) FROM table_name_23 WHERE state = \"iowa\" AND evening_gown < 9.625`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-----------------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Prompt:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```Can you tell me the High assists that has the Date of november 25?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_44 (high_assists VARCHAR, date VARCHAR)```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Original Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```INSERT INTO table_name_44 (high_assists, date) VALUES ('Luka Doncic', 'November 25, 2020')```\n",
       "\n",
       "```INSERT INTO table_name_44 (high_assists, date) VALUES ('Luka Doncic', 'November 25, 2020')```\n",
       "\n",
       "### Output:\n",
       "```SELECT high_assists FROM table_name_44 WHERE date = 'November 25,"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Fine-tuned Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Response:\n",
       "```SELECT high_assists FROM table_name_44 WHERE date = \"november 25\";```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Expected Answer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "`SELECT high_assists FROM table_name_44 WHERE date = \"november 25\"`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-----------------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Prompt:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```What is the current venue for the Miami Masters tournament?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_14903081_1 (current_venue VARCHAR, tournament VARCHAR)```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Original Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Training examples:\n",
       "```WHERE tournament = \"Miami Masters\" AND current_venue IS NOT NULL\n",
       "SELECT current_venue FROM table_14903081_1 WHERE tournament = \"Miami Masters\" AND current_venue IS NOT NULL\n",
       "```\n",
       "\n",
       "- The SQL query will be given with syntax-highlighting in between ``````.\n",
       "- Text-to-SQL queries typically require an additional sentence or two to provide context. You can predict this context too, and we"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Fine-tuned Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Response:\n",
       "```SELECT current_venue FROM table_14903081_1 WHERE tournament = \"Miami Masters\";```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Expected Answer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "`SELECT current_venue FROM table_14903081_1 WHERE tournament = \"Miami Masters\"`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-----------------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Prompt:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```Which highest number of Seats has votes of 244,867?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_83 (seats INTEGER, votes VARCHAR)```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Original Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```INSERT INTO table_name_83 (seats, votes)\n",
       "VALUES\n",
       "(2300, '224,568'),\n",
       "(3950, '244,867'),\n",
       "(3800, '185,679')```\n",
       "\n",
       "### Output:\n",
       "```SELECT seats FROM table_name_83\n",
       "WHERE votes = '244,867'\n",
       "ORDER BY seats DESC\n",
       "LIMIT"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Fine-tuned Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Response:\n",
       "```SELECT MAX(seats) FROM table_name_83 WHERE votes = 244 OFFSET 867;```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Expected Answer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "`SELECT MAX(seats) FROM table_name_83 WHERE votes = 244 OFFSET 867`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-----------------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    generate_responses(val_data[i], ft_model=fine_tuned_model, og_model=not_tuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ee167f-332f-47fe-814a-6f47328b08f7",
   "metadata": {},
   "source": [
    "### 5. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fcc16e89-67b3-4950-abef-59a1afe9c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_name = \"gemma7b-ft-lora-sql-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e682914e-1dc3-4c1a-a6b5-e18b4f40cc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84100c085f8491c97887b367769cc76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f246332689ca48069c8d86252ad6616c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88a5ce7d1f44d1c9c70009a13dc41a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b1a02715aa4f16ad9a0a924546ca9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "093883ac99ee4f82bc107603d66f9b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e37ac4958e47f389ba4e634fd6ce45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd6c0c5019f94cca9c537a76425c52f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7cde54adf1410e951feeda76bc8c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/jjovalle99/gemma7b-ft-lora-sql-v2/commit/8e47d7355bda7b2ede0ce7e5569add2395d31781', commit_message='Upload tokenizer', commit_description='', oid='8e47d7355bda7b2ede0ce7e5569add2395d31781', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model & tokenizer\n",
    "fine_tuned_model.push_to_hub(model_save_name)\n",
    "tokenizer.push_to_hub(model_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2488a39e-3a93-465e-8455-50effbb22267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0c494ab3b14030bd5bc201ba6e8929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e379c678d994daab498c92b45edfd84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/jjovalle99/gemma7bit-lora-sql/commit/ede022bae427fbea1269b9c4748e43ce6b7da034', commit_message='gemma7b-ft-lora-sql-v2adapters', commit_description='', oid='ede022bae427fbea1269b9c4748e43ce6b7da034', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save adapters\n",
    "trainer.push_to_hub(model_save_name + \"adapters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
