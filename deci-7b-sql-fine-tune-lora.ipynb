{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be236001-5f80-4a80-9d2d-53eeaaaa42d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar  6 15:34:14 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  | 00000000:00:0B.0 Off |                    0 |\n",
      "| N/A   30C    P0              42W / 300W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4153f9a9-3973-4041-aa6a-0e2e47ccd769",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers datasets accelerate peft huggingface_hub hf_transfer flash-attn trl wandb -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4345e0da-0b32-426c-b49b-94890a933ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "os.environ[\"WANDB_API_KEY\"] = \"05a4b0b8d2f398533477d5ccc94a6e989cfa822f\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"7bsqlmaster\"\n",
    "os.environ[\"WANDB_NAME\"] = \"DeciLM-Finetune\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32585b96-b68f-42d9-9d99-d3201055ddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython.display import Markdown\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, EarlyStoppingCallback\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset \n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8966a1d-54cb-41ee-bf4d-bda33150d2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Bfloat16 avaiable?: True\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Is Bfloat16 avaiable?: {torch.cuda.is_bf16_supported()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ceeb49-b30b-4df8-bc18-cf4398a1daf7",
   "metadata": {},
   "source": [
    "### 1. Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2630aeb3-093f-44b4-a7ad-f7f71b939d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Deci/DeciLM-7B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b988fa55-9822-400b-ab93-8b7714e76981",
   "metadata": {},
   "source": [
    "#### 1.1 Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d457500-d38d-449d-9487-caac26a897b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81a56e864bc47bc8747b1a425823503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/895 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfb3bb1383684f54ae68bfa6dc57a873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_decilm.py:   0%|          | 0.00/576 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489fd00c82e94593af188e91199b8bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "version_check.py:   0%|          | 0.00/383 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Deci/DeciLM-7B:\n",
      "- version_check.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922d4839593b4522b875ab22a22b86a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)sformers_v4_35_2__configuration_llama.py:   0%|          | 0.00/9.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Deci/DeciLM-7B:\n",
      "- transformers_v4_35_2__configuration_llama.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/Deci/DeciLM-7B:\n",
      "- configuration_decilm.py\n",
      "- version_check.py\n",
      "- transformers_v4_35_2__configuration_llama.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "110776e2b2b34a9eae1b2f6ab002042b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_decilm.py:   0%|          | 0.00/14.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6493b3f395f34ac4b652b61322ce05e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "transformers_v4_35_2__modeling_llama.py:   0%|          | 0.00/56.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34a977dce234bd5a247efcbec40894b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)ers_v4_35_2__modeling_attn_mask_utils.py:   0%|          | 0.00/10.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Deci/DeciLM-7B:\n",
      "- transformers_v4_35_2__modeling_attn_mask_utils.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/Deci/DeciLM-7B:\n",
      "- transformers_v4_35_2__modeling_llama.py\n",
      "- transformers_v4_35_2__modeling_attn_mask_utils.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/Deci/DeciLM-7B:\n",
      "- modeling_decilm.py\n",
      "- transformers_v4_35_2__modeling_llama.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3856124c92d4ddcb5bc87316dee70a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54a409defa740488cb5244914531a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8bd8324eb8c4d8895788c66f1b05222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b9b79d67d24e4fa597996c9ecb100d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5642ae7b236417d82a59db27d9a520e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce1d9854a194425b00aa3124ff69b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be250617-869f-43fb-a6a5-7c53b15b000e",
   "metadata": {},
   "source": [
    "#### 1.2 Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f884a61-4768-4bb2-b5e0-316f1f7e3bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93b7fc8c56d1429a89920fc7fc1b6413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94648c7e73e64959bd62dd8948d7dd5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3647c35056f249feb2b5050c9fd41fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fdb9bd43ea94c62a5a2affd0cd88e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    padding_side=\"left\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb2f44c8-66ec-4a25-a8f0-b25c7e40aa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size of DeciLM7B: 32,000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size of DeciLM7B: {len(tokenizer.get_vocab()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f2f21d0-0154-4928-ad8a-f5284fee129d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c066a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a837a-cf38-48a9-9f85-40e8c0249051",
   "metadata": {},
   "source": [
    "#### 1.3 Inferece test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38232e6e-6f3c-4e53-bcce-8bd13f060a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 1,\n",
    "    \"top_k\": 100,\n",
    "    \"top_p\":0.90,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9c4cc8c-dd4a-4e18-b1d1-6a359875a579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Write me a poem about Machine Learning. Make it rhyme and scan.\n",
       "No.\n",
       "If we make a graph with time on the x-axis and values on the y-axis, then the value of the line will tell us how fast our system learns.\n",
       "Well, its kinda sorta like that. This line shows how much progress your system is making.\n",
       "We used to use a single model with our system, so the amount of progress our system was making was defined by how good our single model was. We"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(text=input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**input_ids, **generation_config)\n",
    "Markdown(tokenizer.decode(token_ids=outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b0ed60-bb49-48d1-8f3f-3b28729d1a63",
   "metadata": {},
   "source": [
    "### 2. Train data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d8d3d2-047e-47bc-9f5f-93d03cc46bd6",
   "metadata": {},
   "source": [
    "#### 2.1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07c8a3ba-1be3-4f32-a680-7bad0527528c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15282044d04342f5bb911b88ba667845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/4.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 21.8M/21.8M [00:00<00:00, 65.3MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8df4512129446b81fb7f903a0bdce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86272db-2765-4c5b-82a4-a4b29471f78c",
   "metadata": {},
   "source": [
    "#### 2.2 Split into test and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae92bcd3-095a-431c-9af6-f3808967b71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78477 100\n"
     ]
    }
   ],
   "source": [
    "train_test_split = dataset.train_test_split(test_size=100, seed=1399, shuffle=True)\n",
    "train_data = train_test_split[\"train\"].shuffle()\n",
    "val_data = train_test_split[\"test\"].shuffle()\n",
    "print(len(train_data), len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa447995-9979-4c9d-a94f-dd03bc494a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "sample = train_data[torch.randint(low=0, high=len(train_data), size=(1,)).item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7c6e24-7837-4566-b465-8727d828c097",
   "metadata": {},
   "source": [
    "#### 2.2 Testing baseline inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dce71678-15dc-4d4c-b1ac-cf1b04e5c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\\n\\n\" + \\\n",
    "\"You must output the SQL query that answers the question.\\n\\n\" + \\\n",
    "\"### Input:\\n\" + \\\n",
    "\"```{question}```\\n\\n\" + \\\n",
    "\"### Context:\\n\" + \\\n",
    "\"```{context}```\\n\\n\"\n",
    "# \"### Response:\\n\" + \\\n",
    "# \"```{response}```\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd6699d7-384f-46bc-bef2-519d226cf9f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```What is the type of the player who ended in 2013, had a summer transfer window, and moved from kalmar ff?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_34 (type VARCHAR, moving_from VARCHAR, ends VARCHAR, transfer_window VARCHAR)```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(template.format(question=sample[\"question\"], context=sample[\"context\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b65c74f3-8aa8-4ca1-ae94-6d451f666289",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.format(context=sample[\"context\"], question=sample[\"question\"])\n",
    "input_ids = tokenizer(text=prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**input_ids, **generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3e97399-b003-471f-85e8-41a164753e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```INSERT INTO table_name_34 VALUES ('s', 'a', '2013', 'y')```\n",
       "\n",
       "```INSERT INTO table_name_34 VALUES ('s', 'b', '2013', 'n')```\n",
       "\n",
       "```INSERT INTO table_name_34 VALUES ('s', 'c', '2013', 'n')```\n",
       "\n",
       "```INSERT IN"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Answer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "SELECT type FROM table_name_34 WHERE ends = 2013 AND transfer_window = \"summer\" AND moving_from = \"kalmar ff\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(Markdown(\"#### Completion:\"))\n",
    "display(Markdown(tokenizer.decode(token_ids=outputs[0], skip_special_tokens=True).replace(prompt, \"\")))\n",
    "display(Markdown(\"#### Answer:\"))\n",
    "Markdown(sample[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac0bea2-c9da-47c3-b3ea-4cafad52718e",
   "metadata": {},
   "source": [
    "#### 2.3 Creating template function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92050802-48a8-4b1c-9e9d-7010eb7fdcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    template = \"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\\n\\n\" + \\\n",
    "    \"You must output the SQL query that answers the question.\\n\\n\" + \\\n",
    "    \"### Input:\\n\" + \\\n",
    "    \"```{question}```\\n\\n\" + \\\n",
    "    \"### Context:\\n\" + \\\n",
    "    \"```{context}```\\n\\n\" + \\\n",
    "    \"### Response:\\n\" + \\\n",
    "    \"```{answer};```\"\n",
    "\n",
    "    text = template.format(context=example[\"context\"], question=example[\"question\"], answer=example[\"answer\"])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3831a9b6-b1d1-4535-aada-bde7bb3321a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```What is the Ship Type of the Prize Disposition of ship and a tonnage less than 151?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_47 (ship_type VARCHAR, disposition_of_ship VARCHAR, tonnage VARCHAR)```\n",
       "\n",
       "### Response:\n",
       "```SELECT ship_type FROM table_name_47 WHERE disposition_of_ship = \"prize\" AND tonnage < 151;```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(formatting_func(train_data[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c598fe-753d-468f-8791-b16d8fc62b18",
   "metadata": {},
   "source": [
    "### 3. Parameter Efficient Fine-Tuning (PEFT) - LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbfb5331-e91c-4d02-b337-19a01ee9de2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeciLMForCausalLM(\n",
      "  (model): DeciLMModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-4): 5 x DeciLMDecoderLayer(\n",
      "        (self_attn): DeciLMAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaDynamicNTKScalingRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (5-9): 5 x DeciLMDecoderLayer(\n",
      "        (self_attn): DeciLMAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaDynamicNTKScalingRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (10): DeciLMDecoderLayer(\n",
      "        (self_attn): DeciLMAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaDynamicNTKScalingRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (11-19): 9 x DeciLMDecoderLayer(\n",
      "        (self_attn): DeciLMAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaDynamicNTKScalingRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (20-30): 11 x DeciLMDecoderLayer(\n",
      "        (self_attn): DeciLMAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=128, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=128, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaDynamicNTKScalingRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "      (31): DeciLMDecoderLayer(\n",
      "        (self_attn): DeciLMAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): LlamaDynamicNTKScalingRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b64730-1a79-464f-86b8-0726c232424a",
   "metadata": {},
   "source": [
    "#### 3.1 Prepare LoRA Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f90ae01f-1ec9-4ff5-ba09-99ab28c3f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "if model.config.to_dict()[\"use_cache\"]:\n",
    "    model.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42d72839-f62e-4c61-8b7b-3475d2af6dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6788ca75-065a-41c0-a1a7-bd04998cb54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model=model, peft_config=peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fccf984-2137-4e2d-9efa-8de7cd588a3f",
   "metadata": {},
   "source": [
    "#### 3.2 Check trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb19e1d8-5d24-4070-b0c7-c1699422d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63bebb3c-c17b-4721-9836-d333378f58a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 41168896 || all params: 7084720128 || trainable%: 0.5810941752983809\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de297270-16f7-4470-9329-6d86c74fe956",
   "metadata": {},
   "source": [
    "### 4. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc009739-c94d-41e6-b628-b3d34f1c49c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_definition = dict(\n",
    "    output_dir=\"/deci7bit-lora-sql\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=3e-4,\n",
    "    max_steps=500,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm = 0.3,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=20,\n",
    "    save_steps=20,\n",
    "    logging_first_step=True,\n",
    "    seed=1399,\n",
    "    bf16=True,\n",
    "    report_to=\"wandb\",\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "args = TrainingArguments(**args_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38ce2980-bfa0-46ba-88b7-f74c6bbc901f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aeec02079964eca887cecf86c24dc4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7859b19cdc59453e8914bc84a3070651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:294: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    args=args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=1024,\n",
    "    packing=True,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abf601b1-fb2c-47db-b172-83551e936d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjj-ovalle\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20240306_153735-v8066942</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jj-ovalle/7bsqlmaster/runs/v8066942' target=\"_blank\">DeciLM-Finetune</a></strong> to <a href='https://wandb.ai/jj-ovalle/7bsqlmaster' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jj-ovalle/7bsqlmaster' target=\"_blank\">https://wandb.ai/jj-ovalle/7bsqlmaster</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jj-ovalle/7bsqlmaster/runs/v8066942' target=\"_blank\">https://wandb.ai/jj-ovalle/7bsqlmaster/runs/v8066942</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/500 1:01:52 < 41:31, 0.08 it/s, Epoch 0/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.784100</td>\n",
       "      <td>0.520397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.483700</td>\n",
       "      <td>0.437631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.432500</td>\n",
       "      <td>0.412212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.411200</td>\n",
       "      <td>0.402869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.402800</td>\n",
       "      <td>0.392537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.395800</td>\n",
       "      <td>0.385506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.389500</td>\n",
       "      <td>0.381605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.381800</td>\n",
       "      <td>0.378438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.375300</td>\n",
       "      <td>0.375633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.372200</td>\n",
       "      <td>0.373390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.368700</td>\n",
       "      <td>0.370207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.367800</td>\n",
       "      <td>0.366451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.363600</td>\n",
       "      <td>0.362655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.358200</td>\n",
       "      <td>0.358021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.359400</td>\n",
       "      <td>0.359274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.41721357544263205, metrics={'train_runtime': 3725.7388, 'train_samples_per_second': 4.294, 'train_steps_per_second': 0.134, 'total_flos': 4.101428553449472e+17, 'train_loss': 0.41721357544263205, 'epoch': 0.85})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4680fac3-56d5-42ed-be10-ae3b755b094e",
   "metadata": {},
   "source": [
    "#### 4.1 Compare outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6da176e-562c-4f66-a792-e87495f3db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3699db5-f733-4aca-a404-b309bfe28ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.format(context=sample[\"context\"], question=sample[\"question\"])\n",
    "input_ids = tokenizer(text=prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = fine_tuned_model.generate(**input_ids, **generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1a2f4e3b-12a7-44bf-99c8-fef117fb48fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```What is the type of the player who ended in 2013, had a summer transfer window, and moved from kalmar ff?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_34 (type VARCHAR, moving_from VARCHAR, ends VARCHAR, transfer_window VARCHAR)```\n",
       "\n",
       "### Response:\n",
       "```SELECT type FROM table_name_34 WHERE ends = \"2013\" AND transfer_window = \"summer\" AND moving_from = \"kalmar ff\";```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Answer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "SELECT type FROM table_name_34 WHERE ends = 2013 AND transfer_window = \"summer\" AND moving_from = \"kalmar ff\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(Markdown(\"#### Completion:\"))\n",
    "display(Markdown(tokenizer.decode(token_ids=outputs[0], skip_special_tokens=True)))\n",
    "display(Markdown(\"#### Answer:\"))\n",
    "Markdown(sample[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b994a4-35c2-42ff-98d3-bd8ccec79a90",
   "metadata": {},
   "source": [
    "#### 4.2 Performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44877767-2bef-4060-95fc-d2b0e54cc796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c8d3f2b29d4c3e9d8e1f994f116f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "not_tuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "fine_tuned_model.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58ece5f5-a99a-49e9-96b7-50f0f82662d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(example, ft_model, og_model):\n",
    "    prompt = template.format(context=example[\"context\"], question=example[\"question\"])\n",
    "    input_ids = tokenizer(text=prompt, return_tensors=\"pt\").to(device)\n",
    "    ft_outputs = ft_model.generate(**input_ids, **generation_config)\n",
    "    og_outputs = og_model.generate(**input_ids, **generation_config)\n",
    "\n",
    "    display(Markdown(\"#### Prompt:\"))\n",
    "    display(Markdown(prompt))\n",
    "    display(Markdown(\"#### Original Completion:\"))\n",
    "    display(Markdown(tokenizer.decode(token_ids=og_outputs[0], skip_special_tokens=True) \\\n",
    "           .replace(prompt, \"\")))\n",
    "    display(Markdown(\"#### Fine-tuned Completion:\"))\n",
    "    display(Markdown(tokenizer.decode(token_ids=ft_outputs[0], skip_special_tokens=True) \\\n",
    "           .replace(prompt, \"\")))\n",
    "    display(Markdown(\"#### Expected Answer:\"))\n",
    "    display(Markdown(\"`{answer}`\".format(answer=example[\"answer\"])))\n",
    "    display(Markdown(\"-----------------------------\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8066b6e2-f1d3-492c-be51-338a0f56fb90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Prompt:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```What is the Political group for p. maelius capitolinus?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_58 (political_group VARCHAR, name VARCHAR)```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Original Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```INSERT INTO table_name_58 VALUES ('Pomeranian')```\n",
       "\n",
       "```INSERT INTO table_name_58 VALUES ('Pomeranian')```\n",
       "\n",
       "```INSERT INTO table_name_58 VALUES ('Pomeranian')```\n",
       "\n",
       "```INSERT INTO table_name_58 VALUES ('Pomeranian')```\n",
       "\n",
       "```INSERT INTO table_name"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Fine-tuned Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Response:\n",
       "```SELECT political_group FROM table_name_58 WHERE name = \"p. maelius capitolinus\";```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Expected Answer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "`SELECT political_group FROM table_name_58 WHERE name = \"p. maelius capitolinus\"`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-----------------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Prompt:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```Which opponent had the result of W 24-14?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_64 (opponent VARCHAR, result VARCHAR)```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Original Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```SELECT result FROM table_name_64 WHERE opponent = \"W 24-14\";```\n",
       "\n",
       "### Output:\n",
       "```SELECT result FROM table_name_64 WHERE opponent = \"W 24-14\"\n",
       "+---------+\n",
       "| result  |\n",
       "+---------+\n",
       "| W 24-14 |\n",
       "+---------+\n",
       "1 row in set (0.00 sec)``` "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Fine-tuned Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Response:\n",
       "```SELECT opponent FROM table_name_64 WHERE result = \"w 24-14\";```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Expected Answer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "`SELECT opponent FROM table_name_64 WHERE result = \"w 24-14\"`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-----------------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Prompt:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```Who is Doug Basham's team?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_39 (team VARCHAR, wrestler VARCHAR)```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Original Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```INSERT INTO table_name_39 VALUES ('Basham')```\n",
       "\n",
       "```INSERT INTO table_name_39 VALUES ('Basham', 'Doug')```\n",
       "```\n",
       "```\n",
       "### Output:\n",
       "SELECT team FROM table_name_39 WHERE wrestler = 'Doug'"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Fine-tuned Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Response:\n",
       "```SELECT team FROM table_name_39 WHERE wrestler = \"doug basham\";```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Expected Answer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "`SELECT team FROM table_name_39 WHERE wrestler = \"doug basham\"`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-----------------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Prompt:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```What is the name of the Tournament on oct 23?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_36 (tournament VARCHAR, date VARCHAR)```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Original Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```INSERT INTO table_name_36 VALUES ('Tennis', 'Nov 20, 1994')```\n",
       "\n",
       "```INSERT INTO table_name_36 VALUES ('Soccer', 'Oct 23, 1995')```\n",
       "\n",
       "```INSERT INTO table_name_36 VALUES ('Tennis', 'Nov 22, 1993')```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Fine-tuned Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Response:\n",
       "```SELECT tournament FROM table_name_36 WHERE date = \"oct 23\";```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Expected Answer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "`SELECT tournament FROM table_name_36 WHERE date = \"oct 23\"`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-----------------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Prompt:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```What was the record after the January 18 game?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_86 (record VARCHAR, date VARCHAR)```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Original Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```CREATE TABLE table_name_87 (team VARCHAR, win_percent FLOAT, loss_percent FLOAT, record VARCHAR)```\n",
       "\n",
       "```CREATE TABLE table_name_88 (player VARCHAR, field_goals INT, tries_scored INT, tries_converted INT, drop_goals INT, penalty_goals INT, total_points INT)```\n",
       "\n",
       "```CREATE TABLE table_name_8"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Fine-tuned Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Response:\n",
       "```SELECT record FROM table_name_86 WHERE date = \"january 18\";```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Expected Answer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "`SELECT record FROM table_name_86 WHERE date = \"january 18\"`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-----------------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    generate_responses(val_data[i], ft_model=fine_tuned_model, og_model=not_tuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ee167f-332f-47fe-814a-6f47328b08f7",
   "metadata": {},
   "source": [
    "### 5. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fcc16e89-67b3-4950-abef-59a1afe9c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_name = \"deci7b-ft-lora-sql-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e682914e-1dc3-4c1a-a6b5-e18b4f40cc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0e6724dd4c4047871ff9605b55a038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3681757e35b643ba94daf3b8d3d6d14a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f42b6cf12b432e8fa22e5c5783b731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7878dd980aa9439599e09274034e5e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d31d07ff34ea44a0925dc35a0cba69bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/jjovalle99/deci7b-ft-lora-sql-v2/commit/8453ea9d296c448e30b3311d2e74d4017da2be14', commit_message='Upload tokenizer', commit_description='', oid='8453ea9d296c448e30b3311d2e74d4017da2be14', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model & tokenizer\n",
    "fine_tuned_model.push_to_hub(model_save_name)\n",
    "tokenizer.push_to_hub(model_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2488a39e-3a93-465e-8455-50effbb22267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dabbcb473c2f4448bd61e8118fd4bead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "780808895678459d9cd9b8386fc5cf7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/jjovalle99/deci7bit-lora-sql/commit/918ff9a49d3e71d8e7d647859e32f461e01871fa', commit_message='deci7b-ft-lora-sql-v2adapters', commit_description='', oid='918ff9a49d3e71d8e7d647859e32f461e01871fa', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save adapters\n",
    "trainer.push_to_hub(model_save_name + \"adapters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
