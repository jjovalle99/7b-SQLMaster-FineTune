{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be236001-5f80-4a80-9d2d-53eeaaaa42d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar  6 14:20:11 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  | 00000000:00:06.0 Off |                    0 |\n",
      "| N/A   33C    P0              43W / 300W |      4MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4153f9a9-3973-4041-aa6a-0e2e47ccd769",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers datasets accelerate peft huggingface_hub hf_transfer flash-attn trl wandb -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4345e0da-0b32-426c-b49b-94890a933ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"<hftoken>\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "os.environ[\"WANDB_API_KEY\"] = \"<w&bapikey>\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"7bsqlmaster\"\n",
    "os.environ[\"WANDB_NAME\"] = \"Mistral-Finetune\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32585b96-b68f-42d9-9d99-d3201055ddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython.display import Markdown\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, EarlyStoppingCallback\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset \n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8966a1d-54cb-41ee-bf4d-bda33150d2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Bfloat16 avaiable?: True\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Is Bfloat16 avaiable?: {torch.cuda.is_bf16_supported()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ceeb49-b30b-4df8-bc18-cf4398a1daf7",
   "metadata": {},
   "source": [
    "### 1. Load model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2630aeb3-093f-44b4-a7ad-f7f71b939d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b988fa55-9822-400b-ab93-8b7714e76981",
   "metadata": {},
   "source": [
    "#### 1.1 Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d457500-d38d-449d-9487-caac26a897b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933a5e2a92b04c21aa644282a94260fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98470d3891a48a3b0fd1d468b6973d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ffde91e7694c1c9627fe916adff245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c152b6a25d8f4a14bba6a1e2d1e586b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d344f3e3f3a84c8498ac56ee58b299b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39232723ada461180dbd9b82ab88697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66fdb1e514ce4e2bb439690ccc4538dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be250617-869f-43fb-a6a5-7c53b15b000e",
   "metadata": {},
   "source": [
    "#### 1.2 Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f884a61-4768-4bb2-b5e0-316f1f7e3bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d058d3f1b8c749dca4c95d3e5c7d6be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b68ee863f0bb4b4baf2c34f670b5e690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070d869e6e4a45958ded9c0b85330aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bad28ee794cb4df6bc805f5ce78232fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    padding_side=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb2f44c8-66ec-4a25-a8f0-b25c7e40aa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size of Mistral7B: 32,000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size of Mistral7B: {len(tokenizer.get_vocab()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f2f21d0-0154-4928-ad8a-f5284fee129d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c066a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a837a-cf38-48a9-9f85-40e8c0249051",
   "metadata": {},
   "source": [
    "#### 1.3 Inferece test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38232e6e-6f3c-4e53-bcce-8bd13f060a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 1,\n",
    "    \"top_k\": 100,\n",
    "    \"top_p\":0.90,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9c4cc8c-dd4a-4e18-b1d1-6a359875a579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Write me a poem about Machine Learning.\n",
       "\n",
       "I think about words as a language.\n",
       "\n",
       "I think about my body as a language.\n",
       "\n",
       "I think about time as a language.\n",
       "\n",
       "I think about music as a language.\n",
       "\n",
       "I think about dreams as a language.\n",
       "\n",
       "I think about algorithms as a language.\n",
       "\n",
       "I think about writing as a language.\n",
       "\n",
       "I think about poetry as a language.\n",
       "\n",
       "I think about art as a language.\n",
       "\n",
       "I think about my mind as a"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(text=input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**input_ids, **generation_config)\n",
    "Markdown(tokenizer.decode(token_ids=outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b0ed60-bb49-48d1-8f3f-3b28729d1a63",
   "metadata": {},
   "source": [
    "### 2. Train data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d8d3d2-047e-47bc-9f5f-93d03cc46bd6",
   "metadata": {},
   "source": [
    "#### 2.1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07c8a3ba-1be3-4f32-a680-7bad0527528c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86272db-2765-4c5b-82a4-a4b29471f78c",
   "metadata": {},
   "source": [
    "#### 2.2 Split into test and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae92bcd3-095a-431c-9af6-f3808967b71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78477 100\n"
     ]
    }
   ],
   "source": [
    "train_test_split = dataset.train_test_split(test_size=100, seed=1399, shuffle=True)\n",
    "train_data = train_test_split[\"train\"].shuffle()\n",
    "val_data = train_test_split[\"test\"].shuffle()\n",
    "print(len(train_data), len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa447995-9979-4c9d-a94f-dd03bc494a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "sample = train_data[torch.randint(low=0, high=len(train_data), size=(1,)).item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7c6e24-7837-4566-b465-8727d828c097",
   "metadata": {},
   "source": [
    "#### 2.2 Testing baseline inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dce71678-15dc-4d4c-b1ac-cf1b04e5c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\\n\\n\" + \\\n",
    "\"You must output the SQL query that answers the question.\\n\\n\" + \\\n",
    "\"### Input:\\n\" + \\\n",
    "\"```{question}```\\n\\n\" + \\\n",
    "\"### Context:\\n\" + \\\n",
    "\"```{context}```\\n\\n\"\n",
    "# \"### Response:\\n\" + \\\n",
    "# \"```{response}```\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd6699d7-384f-46bc-bef2-519d226cf9f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```What is the sum of Total, when Silver is greater than 1, when Nation is Germany (GER), and when Gold is less than 1?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_87 (total INTEGER, gold VARCHAR, silver VARCHAR, nation VARCHAR)```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(template.format(question=sample[\"question\"], context=sample[\"context\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b65c74f3-8aa8-4ca1-ae94-6d451f666289",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.format(context=sample[\"context\"], question=sample[\"question\"])\n",
    "input_ids = tokenizer(text=prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**input_ids, **generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3e97399-b003-471f-85e8-41a164753e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```table_name_87\n",
       "+----------+------------+-----------+---------+\n",
       "| total    | gold       | silver    | nation  |\n",
       "+----------+------------+-----------+---------+\n",
       "|        2 | No         | Yes       | GER     |\n",
       "|        5 | Yes        | No        | FIN     |\n",
       "|        1 | No         | Yes       | USA     |\n",
       "|        1 | No         | No        | GBR    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Answer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "SELECT SUM(total) FROM table_name_87 WHERE silver > 1 AND nation = \"germany (ger)\" AND gold < 1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(Markdown(\"#### Completion:\"))\n",
    "display(Markdown(tokenizer.decode(token_ids=outputs[0], skip_special_tokens=True).replace(prompt, \"\")))\n",
    "display(Markdown(\"#### Answer:\"))\n",
    "Markdown(sample[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac0bea2-c9da-47c3-b3ea-4cafad52718e",
   "metadata": {},
   "source": [
    "#### 2.3 Creating template function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92050802-48a8-4b1c-9e9d-7010eb7fdcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    template = \"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\\n\\n\" + \\\n",
    "    \"You must output the SQL query that answers the question.\\n\\n\" + \\\n",
    "    \"### Input:\\n\" + \\\n",
    "    \"```{question}```\\n\\n\" + \\\n",
    "    \"### Context:\\n\" + \\\n",
    "    \"```{context}```\\n\\n\" + \\\n",
    "    \"### Response:\\n\" + \\\n",
    "    \"```{answer};```\"\n",
    "\n",
    "    text = template.format(context=example[\"context\"], question=example[\"question\"], answer=example[\"answer\"])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3831a9b6-b1d1-4535-aada-bde7bb3321a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```What college team did Derek Fisher play for?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_19 (school_club_team VARCHAR, player VARCHAR)```\n",
       "\n",
       "### Response:\n",
       "```SELECT school_club_team FROM table_name_19 WHERE player = \"derek fisher\";```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(formatting_func(train_data[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c598fe-753d-468f-8791-b16d8fc62b18",
   "metadata": {},
   "source": [
    "### 3. Parameter Efficient Fine-Tuning (PEFT) - LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fbfb5331-e91c-4d02-b337-19a01ee9de2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralFlashAttention2(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b64730-1a79-464f-86b8-0726c232424a",
   "metadata": {},
   "source": [
    "#### 3.1 Prepare LoRA Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f90ae01f-1ec9-4ff5-ba09-99ab28c3f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "if model.config.to_dict()[\"use_cache\"]:\n",
    "    model.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42d72839-f62e-4c61-8b7b-3475d2af6dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6788ca75-065a-41c0-a1a7-bd04998cb54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model = get_peft_model(model=model, peft_config=peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fccf984-2137-4e2d-9efa-8de7cd588a3f",
   "metadata": {},
   "source": [
    "#### 3.2 Check trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb19e1d8-5d24-4070-b0c7-c1699422d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "63bebb3c-c17b-4721-9836-d333378f58a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 41943040 || all params: 7283675136 || trainable%: 0.5758499550960753\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(peft_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de297270-16f7-4470-9329-6d86c74fe956",
   "metadata": {},
   "source": [
    "### 4. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc009739-c94d-41e6-b628-b3d34f1c49c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_definition = dict(\n",
    "    output_dir=\"/mistral7bit-lora-sql\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=3e-4,\n",
    "    max_steps=500,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm = 0.3,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=20,\n",
    "    save_steps=20,\n",
    "    logging_first_step=True,\n",
    "    seed=1399,\n",
    "    bf16=True,\n",
    "    report_to=\"wandb\",\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "args = TrainingArguments(**args_definition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "38ce2980-bfa0-46ba-88b7-f74c6bbc901f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e5dd0c857e4652856e2eb61d3d3fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e5d3dfe9d2e4090bb2a7b91781c0d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:294: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    args=args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=1024,\n",
    "    packing=True,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "abf601b1-fb2c-47db-b172-83551e936d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjj-ovalle\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20240306_142627-1m8h6vfq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jj-ovalle/7bsqlmaster/runs/1m8h6vfq' target=\"_blank\">Mistral-Finetune</a></strong> to <a href='https://wandb.ai/jj-ovalle/7bsqlmaster' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jj-ovalle/7bsqlmaster' target=\"_blank\">https://wandb.ai/jj-ovalle/7bsqlmaster</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jj-ovalle/7bsqlmaster/runs/1m8h6vfq' target=\"_blank\">https://wandb.ai/jj-ovalle/7bsqlmaster/runs/1m8h6vfq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/500 43:03 < 47:02, 0.09 it/s, Epoch 0/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.753300</td>\n",
       "      <td>0.516929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.480600</td>\n",
       "      <td>0.433820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.428500</td>\n",
       "      <td>0.405471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.403000</td>\n",
       "      <td>0.394427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.396900</td>\n",
       "      <td>0.386949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.389800</td>\n",
       "      <td>0.381256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.383600</td>\n",
       "      <td>0.376557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.378600</td>\n",
       "      <td>0.372552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.370800</td>\n",
       "      <td>0.367515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.368100</td>\n",
       "      <td>0.364338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.362200</td>\n",
       "      <td>0.363088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.362600</td>\n",
       "      <td>0.364029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=240, training_loss=0.4240016654133797, metrics={'train_runtime': 2596.2428, 'train_samples_per_second': 6.163, 'train_steps_per_second': 0.193, 'total_flos': 3.375021593670451e+17, 'train_loss': 0.4240016654133797, 'epoch': 0.68})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4680fac3-56d5-42ed-be10-ae3b755b094e",
   "metadata": {},
   "source": [
    "#### 4.1 Compare outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c6da176e-562c-4f66-a792-e87495f3db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = peft_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3699db5-f733-4aca-a404-b309bfe28ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.format(context=sample[\"context\"], question=sample[\"question\"])\n",
    "input_ids = tokenizer(text=prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = fine_tuned_model.generate(**input_ids, **generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a2f4e3b-12a7-44bf-99c8-fef117fb48fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```What is the sum of Total, when Silver is greater than 1, when Nation is Germany (GER), and when Gold is less than 1?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_87 (total INTEGER, gold VARCHAR, silver VARCHAR, nation VARCHAR)```\n",
       "\n",
       "### Response:\n",
       "```SELECT SUM(total) FROM table_name_87 WHERE silver > 1 AND nation = \"germany (ger)\" AND gold < 1;```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Answer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "SELECT SUM(total) FROM table_name_87 WHERE silver > 1 AND nation = \"germany (ger)\" AND gold < 1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(Markdown(\"#### Completion:\"))\n",
    "display(Markdown(tokenizer.decode(token_ids=outputs[0], skip_special_tokens=True)))\n",
    "display(Markdown(\"#### Answer:\"))\n",
    "Markdown(sample[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b994a4-35c2-42ff-98d3-bd8ccec79a90",
   "metadata": {},
   "source": [
    "#### 4.2 Performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "44877767-2bef-4060-95fc-d2b0e54cc796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44490fe6a38459fa546b802c7ba0e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "not_tuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "fine_tuned_model.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "58ece5f5-a99a-49e9-96b7-50f0f82662d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_responses(example, ft_model, og_model):\n",
    "    prompt = template.format(context=example[\"context\"], question=example[\"question\"])\n",
    "    input_ids = tokenizer(text=prompt, return_tensors=\"pt\").to(device)\n",
    "    ft_outputs = ft_model.generate(**input_ids, **generation_config)\n",
    "    og_outputs = og_model.generate(**input_ids, **generation_config)\n",
    "\n",
    "    display(Markdown(\"#### Prompt:\"))\n",
    "    display(Markdown(prompt))\n",
    "    display(Markdown(\"#### Original Completion:\"))\n",
    "    display(Markdown(tokenizer.decode(token_ids=og_outputs[0], skip_special_tokens=True) \\\n",
    "           .replace(prompt, \"\")))\n",
    "    display(Markdown(\"#### Fine-tuned Completion:\"))\n",
    "    display(Markdown(tokenizer.decode(token_ids=ft_outputs[0], skip_special_tokens=True) \\\n",
    "           .replace(prompt, \"\")))\n",
    "    display(Markdown(\"#### Expected Answer:\"))\n",
    "    display(Markdown(\"`{answer}`\".format(answer=example[\"answer\"])))\n",
    "    display(Markdown(\"-----------------------------\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8066b6e2-f1d3-492c-be51-338a0f56fb90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Prompt:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```What Team has the Engine of Chevrolet 6.0 V8, the Manufacturer of Opel, and the Driver Chris Jackson?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_51 (team VARCHAR, driver VARCHAR, engine VARCHAR, manufacturer VARCHAR)```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Original Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "`table_name_51` has the following content:\n",
       "```\n",
       "team | driver | engine | manufacturer\n",
       "---- | ------ | ------ | ------------\n",
       "Porsche | Chris Jackson | Porsche 6.0 V12 | Porsche\n",
       "McLaren | Lando Norris | Mercedes-AMG 1.6 V6 | McLaren\n",
       "Aston Martin | Daniel Ricciardo | Mercedes-AMG 1.6 V"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Fine-tuned Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Response:\n",
       "```SELECT team FROM table_name_51 WHERE engine = \"chevrolet 6.0 v8\" AND manufacturer = \"opel\" AND driver = \"chris jackson\";```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Expected Answer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "`SELECT team FROM table_name_51 WHERE engine = \"chevrolet 6.0 v8\" AND manufacturer = \"opel\" AND driver = \"chris jackson\"`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-----------------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Prompt:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```What record was set by walter brennan before 1941?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_30 (record_set VARCHAR, actor VARCHAR, year VARCHAR)```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Original Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Output:\n",
       "```SELECT record_set, year FROM table_name_30 WHERE actor = \"Walter Brennan\" AND year < \"1941\"```\n",
       "\n",
       "### Additional context (not necessary, but helpful):\n",
       "```SELECT * FROM table_name_30 WHERE actor = \"Walter Brennan\"```\n",
       "\n",
       "\n",
       "```SELECT * FROM table_name_30 WHERE year = \"1941\"```\n",
       "\n",
       "## Examples"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Fine-tuned Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Response:\n",
       "```SELECT record_set FROM table_name_30 WHERE actor = \"walter brennan\" AND year < 1941;```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Expected Answer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "`SELECT record_set FROM table_name_30 WHERE actor = \"walter brennan\" AND year < 1941`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-----------------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Prompt:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```What album came out before 2008 called We're not Made in the USA?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_93 (album VARCHAR, year VARCHAR, title VARCHAR)```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Original Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Answer:\n",
       "```SELECT * FROM table_name_93 WHERE year < '2008' AND title = 'We're not Made in the USA'```\n",
       "\n",
       "\n",
       "## Install\n",
       "```npm install ask-me```\n",
       "\n",
       "## Usage\n",
       "\n",
       "```\n",
       "const askMe = require(\"ask-me\");\n",
       "```\n",
       "\n",
       "```\n",
       "const q = \"What album came out before 2008 called We'"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Fine-tuned Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Response:\n",
       "```SELECT album FROM table_name_93 WHERE year < 2008 AND title = \"we're not made in the usa\";```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Expected Answer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "`SELECT album FROM table_name_93 WHERE year < 2008 AND title = \"we're not made in the usa\"`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-----------------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Prompt:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```What was the location and attendance when the record was 19-17?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_69 (location_attendance VARCHAR, record VARCHAR)```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Original Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```INSERT INTO table_name_69 (location_attendance, record) VALUES('19-17', '12-29')```\n",
       "\n",
       "```INSERT INTO table_name_69 (location_attendance, record) VALUES('19-19', '14-28')```\n",
       "\n",
       "```INSERT INTO table_name_69 (location_attendance, record) VALUES('"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Fine-tuned Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Response:\n",
       "```SELECT location_attendance FROM table_name_69 WHERE record = \"19-17\";```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Expected Answer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "`SELECT location_attendance FROM table_name_69 WHERE record = \"19-17\"`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-----------------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Prompt:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
       "\n",
       "You must output the SQL query that answers the question.\n",
       "\n",
       "### Input:\n",
       "```What is the maximum power at rpm for the engine named 2.0 TDI that has a 1968cc displacement?```\n",
       "\n",
       "### Context:\n",
       "```CREATE TABLE table_name_90 (max_power_at_rpm VARCHAR, displacement VARCHAR, engine_name VARCHAR)```\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Original Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Output:\n",
       "```SELECT max_power_at_rpm FROM table_name_90 WHERE engine_name = '2.0 TDI' AND displacement = '1968cc'```\n",
       "\n",
       "### Hint:\n",
       "You can ask your prompt the following questions:\n",
       "- What is the max power at rpm?\n",
       "- What is the displacement?\n",
       "- What is the engine name?\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "You are a powerful text"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Fine-tuned Completion:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Response:\n",
       "```SELECT max_power_at_rpm FROM table_name_90 WHERE displacement = \"1968cc\" AND engine_name = \"2.0 tdi\";```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Expected Answer:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "`SELECT max_power_at_rpm FROM table_name_90 WHERE displacement = \"1968cc\" AND engine_name = \"2.0 tdi\"`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "-----------------------------"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    generate_responses(val_data[i], ft_model=fine_tuned_model, og_model=not_tuned_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ee167f-332f-47fe-814a-6f47328b08f7",
   "metadata": {},
   "source": [
    "### 5. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fcc16e89-67b3-4950-abef-59a1afe9c3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_name = \"mistral7b-ft-lora-sql-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e682914e-1dc3-4c1a-a6b5-e18b4f40cc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19f75901b874818969d3f965dd407a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e419e820758d4d08862029a71b939efe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4efbb6c9f24e0ca8284e70aebc0d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c6d812e78b45ee8ba4daf3e4c57a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e024719367e404192f5685d4d0fb648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/jjovalle99/mistral7b-ft-lora-sql-v2/commit/c64bd2a19a22fb886406100e6607fb3a5e2a2144', commit_message='Upload tokenizer', commit_description='', oid='c64bd2a19a22fb886406100e6607fb3a5e2a2144', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model & tokenizer\n",
    "fine_tuned_model.push_to_hub(model_save_name)\n",
    "tokenizer.push_to_hub(model_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2488a39e-3a93-465e-8455-50effbb22267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f185e69d5a3d45feaf3ebb5468ef3ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8392361f6e86454ca4c3d7d6e3731d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/jjovalle99/mistral7bit-lora-sql/commit/6cbea05a7da64b9476e0610e2ed5d17055cd7a1d', commit_message='mistral7b-ft-lora-sql-v2adapters', commit_description='', oid='6cbea05a7da64b9476e0610e2ed5d17055cd7a1d', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save adapters\n",
    "trainer.push_to_hub(model_save_name + \"adapters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
